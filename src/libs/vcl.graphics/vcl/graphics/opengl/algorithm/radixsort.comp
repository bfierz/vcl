/*
 * This file is part of the Visual Computing Library (VCL) release under the
 * MIT license.
 *
 * Copyright (c) 2016 Basil Fierz
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */
namespace
{
const char* module = R"glsl(

#version 430 core

// Local layout
layout(local_size_x = WORKGROUP_SIZE, local_size_y = 1, local_size_z = 1) in;

#if defined radixSortBlocksKeysOnly

// Output data
layout(std430, binding = 0) writeonly buffer KeysOut
{
	uvec4 keysOut[];
};

// Input data
layout(std430, binding = 1) readonly buffer KeysIn
{
	uvec4 keysIn[];
};

// Kernel parameters
uniform uint nbits;
uniform uint startbit;

// Shared data
shared uint l_numTrue;

// Local data
uvec4 key;

uvec4 rank4(uvec4 preds)
{
	uint localId   = gl_LocalInvocationID.x;
	uint localSize = gl_WorkGroupSize.x;

	//uvec4 address = scan4(preds);
	uvec4 address = scan4Exclusive(preds, localSize * 4);
	
	if (localId == localSize - 1) 
	{
		l_numTrue = address.w + preds.w;
	}
    memoryBarrierShared();
	barrier();
	
	uvec4 rank;
	uint idx = localId*4;
	rank.x = (preds.x != 0) ? address.x : l_numTrue + idx + 0 - address.x;
	rank.y = (preds.y != 0) ? address.y : l_numTrue + idx + 1 - address.y;
	rank.z = (preds.z != 0) ? address.z : l_numTrue + idx + 2 - address.z;
	rank.w = (preds.w != 0) ? address.w : l_numTrue + idx + 3 - address.w;
	
	return rank;
}

void radixSortBlockKeysOnly(uint nbits, uint startbit)
{
	uint localId   = gl_LocalInvocationID.x;
    uint localSize = gl_WorkGroupSize.x;
	
	for (uint shift = startbit; shift < (startbit + nbits); ++shift)
	{
		uvec4 lsb;
		lsb.y = ((key.y >> shift) & 0x1) ^ 0x1;
		lsb.x = ((key.x >> shift) & 0x1) ^ 0x1;
        lsb.z = ((key.z >> shift) & 0x1) ^ 0x1;
        lsb.w = ((key.w >> shift) & 0x1) ^ 0x1;
        
		uvec4 r = rank4(lsb);

        // This arithmetic strides the ranks across 4 CTA_SIZE regions
        l_data[(r.x & 3) * localSize + (r.x >> 2)] = key.x;
        l_data[(r.y & 3) * localSize + (r.y >> 2)] = key.y;
        l_data[(r.z & 3) * localSize + (r.z >> 2)] = key.z;
        l_data[(r.w & 3) * localSize + (r.w >> 2)] = key.w;
		memoryBarrierShared();
		barrier();

        // The above allows us to read without 4-way bank conflicts:
        key.x = l_data[localId + 0 * localSize];
        key.y = l_data[localId + 1 * localSize];
        key.z = l_data[localId + 2 * localSize];
        key.w = l_data[localId + 3 * localSize];
		
		memoryBarrierShared();
		barrier();
	}
}

void main()
{
	// Store the keys in private memory in order to manipulate
	// them in the subsequent function
	key = keysIn[gl_GlobalInvocationID.x];

	// Sort the keys within this block
	radixSortBlockKeysOnly(nbits, startbit);
	
	// Store the keys again
	keysOut[gl_GlobalInvocationID.x] = key;
}

#elif defined findRadixOffsets

//----------------------------------------------------------------------------
// Given an array with blocks sorted according to a 4-bit radix group, each 
// block counts the number of keys that fall into each radix in the group, and 
// finds the starting offset of each radix in the block.  It then writes the radix 
// counts to the counters array, and the starting offsets to the blockOffsets array.
//
// Template parameters are used to generate efficient code for various special cases
// For example, we have to handle arrays that are a multiple of the block size 
// (fullBlocks) differently than arrays that are not. "loop" is used when persistent 
// CTAs are used. 
//
// By persistent CTAs we mean that we launch only as many thread blocks as can 
// be resident in the GPU and no more, rather than launching as many threads as
// we have elements. Persistent CTAs loop over blocks of elements until all work
// is complete.  This can be faster in some cases.  In our tests it is faster
// for large sorts (and the threshold is higher on compute version 1.1 and earlier
// GPUs than it is on compute version 1.2 GPUs.
//                                
//----------------------------------------------------------------------------
// Output data
layout(std430, binding = 0) writeonly buffer Counters
{
  uint counters[];
};
layout(std430, binding = 1) writeonly buffer BlockOffsets
{
  uint blockOffsets[];
};

// Input data
layout(std430, binding = 2) readonly buffer Keys
{
  uvec2 keys[];
};

// Kernel parameters
uniform uint startbit;
uniform uint totalBlocks;

// Shared data
shared uint l_radix1[2 * WORKGROUP_SIZE];
shared uint l_startPointers[16];

void main()
{
    uint localId   = gl_LocalInvocationID.x;
	uint globalId  = gl_GlobalInvocationID.x;
    uint groupId   = gl_WorkGroupID.x;
    uint groupSize = gl_WorkGroupSize.x;

    uvec2 radix2 = keys[globalId];

    l_radix1[2 * localId + 0] = (radix2.x >> startbit) & 0xF;
    l_radix1[2 * localId + 1] = (radix2.y >> startbit) & 0xF;

    // Finds the position where the l_radix1 entries differ and stores start 
    // index for each radix.
    if (localId < 16) 
    {
        l_startPointers[localId] = 0; 
    }
    memoryBarrierShared();
	barrier();

    if ((localId > 0) && (l_radix1[localId] != l_radix1[localId - 1])) 
    {
        l_startPointers[l_radix1[localId]] = localId;
    }
    memoryBarrierShared();
	barrier();
	
    if (l_radix1[localId + groupSize] != l_radix1[localId + groupSize - 1]) 
    {
        l_startPointers[l_radix1[localId + groupSize]] = localId + groupSize;
    }
    memoryBarrierShared();
	barrier();

    if (localId < 16) 
    {
        blockOffsets[16 * groupId + localId] = l_startPointers[localId];
    }
    memoryBarrierShared();
	barrier();

    // Compute the sizes of each block.
    if ((localId > 0) && (l_radix1[localId] != l_radix1[localId - 1])) 
    {
        l_startPointers[l_radix1[localId - 1]] = 
            localId - l_startPointers[l_radix1[localId - 1]];
    }
    memoryBarrierShared();
	barrier();

    if (l_radix1[localId + groupSize] != l_radix1[localId + groupSize - 1]) 
    {
        l_startPointers[l_radix1[localId + groupSize - 1]] = 
            localId + groupSize - l_startPointers[l_radix1[localId + groupSize - 1]];
    }
    memoryBarrierShared();
	barrier();

    if (localId == groupSize - 1) 
    {
        l_startPointers[l_radix1[2 * groupSize - 1]] = 
            2 * groupSize - l_startPointers[l_radix1[2 * groupSize - 1]];
    }
    memoryBarrierShared();
	barrier();

    if (localId < 16) 
    {
        counters[localId * totalBlocks + groupId] = l_startPointers[localId];
    }
}

#elif defined reorderDataKeysOnly
//----------------------------------------------------------------------------
// reorderData shuffles data in the array globally after the radix offsets 
// have been found. On compute version 1.1 and earlier GPUs, this code depends 
// on RadixSort::CTA_SIZE being 16 * number of radices (i.e. 16 * 2^nbits).
// 
// On compute version 1.1 GPUs ("manualCoalesce=true") this function ensures
// that all writes are coalesced using extra work in the kernel.  On later
// GPUs coalescing rules have been relaxed, so this extra overhead hurts 
// performance.  On these GPUs we set manualCoalesce=false and directly store
// the results.
//
// Template parameters are used to generate efficient code for various special cases
// For example, we have to handle arrays that are a multiple of the block size 
// (fullBlocks) differently than arrays that are not.  "loop" is used when persistent 
// CTAs are used. 
//
// By persistent CTAs we mean that we launch only as many thread blocks as can 
// be resident in the GPU and no more, rather than launching as many threads as
// we have elements. Persistent CTAs loop over blocks of elements until all work
// is complete.  This can be faster in some cases.  In our tests it is faster
// for large sorts (and the threshold is higher on compute version 1.1 and earlier
// GPUs than it is on compute version 1.2 GPUs.
//----------------------------------------------------------------------------
// Output data
layout(std430, binding = 0) writeonly buffer OutKeys
{
  uint outKeys[];
};

// Input data
layout(std430, binding = 1) readonly buffer Keys
{
  uvec2 keys[];
};
layout(std430, binding = 2) readonly buffer BlockOffsets
{
  uint blockOffsets[];
};
layout(std430, binding = 3) readonly buffer Offsets
{
  uint offsets[];
};

// Kernel parameters
uniform uint startbit;
uniform uint numElements;
uniform uint totalBlocks;

// Shared data
shared uint l_keys[2 * WORKGROUP_SIZE];
shared uint l_offsets[16];
shared uint l_blockOffsets[16];

void main()
{
    uint groupSize = gl_WorkGroupSize.x;
    uint groupId   = gl_WorkGroupID.x;

	uint globalId  = gl_GlobalInvocationID.x;
    uint localId   = gl_LocalInvocationID.x;

    l_keys[2*localId+0] = keys[globalId].x;
    l_keys[2*localId+1] = keys[globalId].y;

    if (localId < 16)  
    {
        l_offsets[localId]      = offsets[localId * totalBlocks + groupId];
        l_blockOffsets[localId] = blockOffsets[groupId * 16 + localId];
    }
	memoryBarrierShared();
	barrier();
	
	uint key = l_keys[localId];
    uint radix = (key >> startbit) & 0xF;
    uint globalOffset = l_offsets[radix] + localId - l_blockOffsets[radix];
    if (globalOffset < numElements)
    {
		outKeys[globalOffset]   = key;
    }
	
	key = l_keys[localId + groupSize];
    radix = (key >> startbit) & 0xF;
    globalOffset = l_offsets[radix] + localId + groupSize - l_blockOffsets[radix];
    if (globalOffset < numElements)
    {
        outKeys[globalOffset]   = key;
    }
}
#endif
)glsl";
}
